{"cells":[{"metadata":{"_uuid":"ae9c853125efd1de77c47d9d6f0d25f78bd44626"},"cell_type":"markdown","source":"# Convolutional Neural Networks (CNN) - Keras\n\ncontent is from A-Z Datascience course\n\n## Content:\n### Explanation\n1. Convolution operation\n2. ReLU\n3. Max Pooling\n4. Flattening\n5. Full-connection\nSoftmax & Cross entropy\n\n### IMPLEMENTATION\n1. Data preprocessing\n2. Build the Keras model\n3. Compile and fit the model\n4. Make predictions and determine accuracy\n\n--------- "},{"metadata":{"_uuid":"96a7ec51310cd0b1fe3545b5ac43a83a9bac8a76"},"cell_type":"markdown","source":"Below are key concepts used in Convolutional Neural Networks.\n\n# Convolution Operation\nThe aim of convolution operation is to reduce the size of an image, by using feature detectors that keep only the specific patterns within the image. Stride is the number of pixels with which we slide the detector. If it is one, we are moving it one pixel each time and recording the value (adding up all the multiplied values). Many feature detectors are used, and the algorithm finds out what is the optimal way to filter images. 3 x 3 feature detector is commonly used, but other sizes can be used.\n\n# ReLU\nAfter feature detectors are applied upon images, ReLU is used to increase non-linearity within images. \n\n# Max Pooling\nTake a 2 x 2 box on the top left corner (starting here), and record the maximum number within the box. Slide it to the right with the stride of 2 (commonly used), and move onto the next row if completed. Repeat this step until all the pixels are evaluated. Aim of max pooling is to keep all the important features even if images have spatial or textual distortions, and also reduce the size which prevents overfitting. So, after applying convolution operation to images, than pooling is applied.\n\nOther pooling techniques are also available such as Mean Pooling, which takes the average of pixels within the box.\n\n# Flattening\nFlatten the matrix into a long vector which will be the input to the artificial neural network\n\n# Full Connection\nImplement full Artificial Neural Network model to optimize weights.\n\n# Softmax & Cross entropy\nSoftmax function brings all predicted values to be between 0 and 1, and make them add up to 1. It also comes hand-in-hand with cross-entropy method. \n\nJust seeing how many wrong predictions the classifier made is not enough to evaluate the performance of ANNs. Instead, Cross Entropy should be used to measure how good the model is, as there can be two models that produce same results while one produced better percentages than the other. For classificaion, Cross Entropy should be used, and for regression, Mean Squared Error should be used. \n\n---\n\nWe will create a dog vs cat classifier. To be able to work with keras library, we need proper structure of images. There should be two folders: Test set and Train set. And, in each folder, cat images and dog images should be placed in two separate folders. In this way, keras will understand how to work with them.\n"},{"metadata":{},"cell_type":"markdown","source":"## 0. Read in Data"},{"metadata":{},"cell_type":"markdown","source":"Data augmentation prevents overfitting, by generating more samples of the images through flipping, rotating, distorting, etc. Keras has built-in Image Augmentation function. To learn more about this function, refer to this [guide](https://keras.io/preprocessing/image/). "},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n# https://keras.io/api/preprocessing/image/\n    \ntrain_datagen = ImageDataGenerator(rescale = 1./255, #normalizing ==> values now 0~1\n                                   shear_range = 0.2, # data augmentation\n                                   zoom_range = 0.2, # data augmentation\n                                   horizontal_flip = True) # data augmentation\n\n\ntraining_set = train_datagen.flow_from_directory('../input/training_set', \ntarget_size = (64, 64), # Tuple of integers (height, width), \n                        # defaults to (256, 256). The dimensions to which all images found will be resized.\nbatch_size = 32, # Size of the batches of data (default: 32).\nclass_mode = 'binary')\n# One of \"categorical\", \"binary\", \"sparse\", \"input\", or None. \n# Default: \"categorical\". Determines the type of label arrays that are returned: \n#         \"categorical\" will be 2D one-hot encoded labels, - \"binary\" will be 1D binary labels, \n#         \"sparse\" will be 1D integer labels, \n#         \"input\" will be images identical to input images (mainly used to work with autoencoders).\n#         If None, no labels are returned (the generator will only yield batches of image data, \n#                                          which is useful to use with model.predict())\n\n######### ######### ######### ######### ######### ######### ######### ######### \n\ntest_datagen = ImageDataGenerator(rescale = 1./255)\n\ntest_set = test_datagen.flow_from_directory('../input/test_set',\n                                                target_size = (64, 64),\n                                                 batch_size = 32, \n                                                 class_mode = 'binary')","execution_count":8,"outputs":[{"output_type":"stream","text":"Found 8005 images belonging to 1 classes.\nFound 2023 images belonging to 1 classes.\n","name":"stdout"}]},{"metadata":{"_uuid":"dd72cab91e1527fc88e2aa80bf23c706e6d351a4"},"cell_type":"markdown","source":"## 1. Build the CNN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.__version__","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"'1.13.1'"},"metadata":{}}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D #images are two dimensional. Videos are three dimenstional with time.\nfrom tensorflow.keras.layers import MaxPool2D\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\n\n#initialize the classifier CNN\nclassifier = Sequential() #Please note that there is another way to build a mode: Functional API.\n\n#applying convolution operation --> build the convolutional layer\nclassifier.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n\n#32, 3, 3 --> 32 filters with 3 x 3 for each filter. \n#start with 32 filters, and then create more layers with 64, 128, 256, etc\n#expected format of the images.\n# 256, 256, 3 --> 3 color channels (RGB), 256 x 256 pixels. But when using CPU, 3, 64, 64 --> due to computational limitation\n\n\n###################### KERAS Implementation ##############################\n# from keras.models import Sequential\n# from keras.layers import Convolution2D #images are two dimensional. Videos are three dimenstional with time.\n# from keras.layers import MaxPooling2D\n# from keras.layers import Flatten\n# from keras.layers import Dense\n\n# #initialize the classifier CNN\n# classifier = Sequential() #Please note that there is another way to build a mode: Functional API.\n\n# #applying convolution operation --> build the convolutional layer\n# classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n\n# #32, 3, 3 --> 32 filters with 3 x 3 for each filter. \n# #start with 32 filters, and then create more layers with 64, 128, 256, etc\n# #expected format of the images.\n# # 256, 256, 3 --> 3 color channels (RGB), 256 x 256 pixels. But when using CPU, 3, 64, 64 --> due to computational limitation","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Max Pooling --> create a pooling layer\nclassifier.add(MaxPool2D(pool_size=2, strides=2)) # Keras: classifier.add(MaxPooling2D(pool_size = (2,2)))\n# 2 x 2 size --> commonly used to keep much information.\n\n#add second convolutional layer\nclassifier.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\nclassifier.add(MaxPool2D(pool_size=2, strides=2))\n\n#Flattening --> creating a long vector.\nclassifier.add(Flatten()) #Keras: classifier.add(Flatten()) #no parameters needed.\n\n#classic ANN - full connection\nclassifier.add(Dense(units=128, activation='relu')) #keras: classifier.add(Dense(output_dim = 128, activation = 'relu'))\n#common practice: number of hidden nodes between the number of input nodes and output nodes, and choose powers of 2\nclassifier.add(Dense(units = 1, activation = 'sigmoid')) #keras: classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"e5a242550e6a54125bc5956ca7a413623520278a"},"cell_type":"markdown","source":"## 2. Compile the model"},{"metadata":{"trusted":true,"_uuid":"3d7430e8c84ffdc7aefce9e42a17ac9e64f4708b"},"cell_type":"code","source":"classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n# this line of code is same as keras implementation","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"1e6d69f6b5539ec603e569b97f00009dec8215f7"},"cell_type":"markdown","source":"## 3. Fit the model on images, image preprocessing"},{"metadata":{"trusted":true,"_uuid":"e1024dd333b75a2e18db3ff0390629d4fcdfcc1d"},"cell_type":"code","source":"classifier.fit(x = training_set, validation_data = test_set, epochs = 25)\n\n### Keras ####\n# classifier.fit_generator(training_set, \n#                          samples_per_epoch = 8005, \n#                         nb_epoch = 2, \n#                         validation_data = test_set, \n#                         nb_val_samples = 2025)","execution_count":13,"outputs":[{"output_type":"stream","text":"Epoch 1/25\n64/64 [==============================] - 15s 235ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 82s 329ms/step - loss: 0.0026 - acc: 0.9999 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 2/25\n64/64 [==============================] - 7s 104ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 152ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 3/25\n64/64 [==============================] - 7s 103ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 37s 147ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 4/25\n64/64 [==============================] - 6s 101ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 37s 149ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 5/25\n64/64 [==============================] - 7s 114ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 152ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 6/25\n64/64 [==============================] - 7s 117ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 37s 147ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 7/25\n64/64 [==============================] - 7s 104ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 151ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 8/25\n64/64 [==============================] - 7s 116ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 37s 149ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 9/25\n64/64 [==============================] - 6s 100ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 37s 148ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 10/25\n64/64 [==============================] - 7s 104ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 151ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 11/25\n64/64 [==============================] - 7s 107ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 37s 148ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 12/25\n64/64 [==============================] - 7s 104ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 150ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 13/25\n64/64 [==============================] - 7s 107ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 151ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 14/25\n64/64 [==============================] - 6s 101ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 37s 146ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 15/25\n64/64 [==============================] - 7s 106ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 151ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 16/25\n64/64 [==============================] - 7s 113ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 150ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 17/25\n64/64 [==============================] - 7s 114ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 39s 154ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 18/25\n64/64 [==============================] - 6s 100ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 151ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 19/25\n64/64 [==============================] - 7s 105ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 37s 148ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 20/25\n64/64 [==============================] - 7s 106ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 151ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 21/25\n64/64 [==============================] - 7s 108ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 152ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 22/25\n64/64 [==============================] - 7s 105ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 37s 147ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 23/25\n64/64 [==============================] - 7s 109ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 152ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 24/25\n64/64 [==============================] - 7s 111ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 38s 152ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\nEpoch 25/25\n64/64 [==============================] - 6s 101ms/step - loss: 1.0000e-07 - acc: 1.0000\n251/251 [==============================] - 37s 149ms/step - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n","name":"stdout"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fade70fc208>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 4. Making a single prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import numpy as np\n# from keras.preprocessing import image\n# test_image = image.load_img('../input/cat-and-dog/test_set/cats/cat.4021.jpg',\n#                             target_size = (64, 64))\n\n# test_image = image.img_to_array(test_image)\n# test_image = np.expand_dims(test_image, axis = 0)\n\n# result = cnn.predict(test_image)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training_set.class_indices","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if result[0][0] == 1:\n#     prediction = 'dog'\n# else:\n#     prediction = 'cat'","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a7935038590e94e3f9af509f1b09d6d729e07c5"},"cell_type":"markdown","source":"## 4. Improving the model\nThere are two possible ways of reducing variane, which is making the model fit more to the train set.\n* Add more convolutional layers \n    * This will allow more features to be detected prior to fitting to ANN. Make sure to not include input_dim, and include MaxPooling step. Flattening should be at the end of all the layers.\n* Add more fully-connected layer (hidden layers)\n    * Catches more complex behaviors\n    \nThank you for reading this kernel. If you found this helpful, please upvote the kernel or put a short comment below. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}