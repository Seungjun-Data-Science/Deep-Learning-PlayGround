{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Multi Variable Linear Regression with TF"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Read in Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\nprint(tf.__version__)","execution_count":1,"outputs":[{"output_type":"stream","text":"2.2.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Two Variable Example of MLR with TF"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(0)  # for reproducibility","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1_data = [1, 0, 3, 0, 5]\nx2_data = [0, 2, 0, 4, 0]\ny_data  = [1, 2, 3, 4, 5]\n\nW1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\nW2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\nb  = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n\nlearning_rate = tf.Variable(0.001)\n\nfor i in range(1000+1):\n    with tf.GradientTape() as tape:\n        hypothesis = W1 * x1_data + W2 * x2_data + b\n        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n    W1.assign_sub(learning_rate * W1_grad)\n    W2.assign_sub(learning_rate * W2_grad)\n    b.assign_sub(learning_rate * b_grad)\n\n    if i % 50 == 0:\n        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))","execution_count":3,"outputs":[{"output_type":"stream","text":"    0 | 335.280823 |    -4.0663 |     1.1220 |  -6.065215\n   50 |  76.037262 |    -0.8001 |     1.6209 |  -4.978779\n  100 |  18.959263 |     0.7151 |     1.8781 |  -4.429109\n  150 |   6.310240 |     1.4125 |     2.0104 |  -4.134423\n  200 |   3.445082 |     1.7284 |     2.0768 |  -3.961648\n  250 |   2.743659 |     1.8667 |     2.1075 |  -3.847750\n  300 |   2.525401 |     1.9225 |     2.1184 |  -3.762738\n  350 |   2.417754 |     1.9402 |     2.1181 |  -3.692262\n  400 |   2.337300 |     1.9403 |     2.1114 |  -3.629400\n  450 |   2.264998 |     1.9325 |     2.1008 |  -3.570778\n  500 |   2.196328 |     1.9213 |     2.0881 |  -3.514729\n  550 |   2.130126 |     1.9085 |     2.0741 |  -3.460409\n  600 |   2.066037 |     1.8953 |     2.0595 |  -3.407385\n  650 |   2.003917 |     1.8819 |     2.0444 |  -3.355424\n  700 |   1.943679 |     1.8686 |     2.0293 |  -3.304398\n  750 |   1.885258 |     1.8555 |     2.0141 |  -3.254230\n  800 |   1.828595 |     1.8425 |     1.9990 |  -3.204873\n  850 |   1.773636 |     1.8297 |     1.9841 |  -3.156293\n  900 |   1.720329 |     1.8171 |     1.9693 |  -3.108468\n  950 |   1.668625 |     1.8048 |     1.9547 |  -3.061379\n 1000 |   1.618474 |     1.7926 |     1.9403 |  -3.015011\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Two Variable(with matrix) Example of MLR with TF"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_data = [\n    [1., 0., 3., 0., 5.],\n    [0., 2., 0., 4., 0.]\n]\ny_data  = [1, 2, 3, 4, 5]\n\nW = tf.Variable(tf.random.uniform((1, 2), -1.0, 1.0))\nb = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))\n\nlearning_rate = tf.Variable(0.001)\n\nfor i in range(1000+1):\n    with tf.GradientTape() as tape:\n        hypothesis = tf.matmul(W, x_data) + b # (1, 2) * (2, 5) = (1, 5)\n        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n\n        W_grad, b_grad = tape.gradient(cost, [W, b])\n        W.assign_sub(learning_rate * W_grad)\n        b.assign_sub(learning_rate * b_grad)\n    \n    if i % 50 == 0:\n        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))","execution_count":4,"outputs":[{"output_type":"stream","text":"    0 |  36.403778 |    -0.6231 |    -0.3508 |  -0.961774\n   50 |   9.372901 |     0.2914 |     0.1682 |  -0.557764\n  100 |   2.639858 |     0.7060 |     0.4867 |  -0.347756\n  150 |   0.825069 |     0.8912 |     0.6846 |  -0.235665\n  200 |   0.284990 |     0.9721 |     0.8088 |  -0.174012\n  250 |   0.106844 |     1.0062 |     0.8873 |  -0.138953\n  300 |   0.042677 |     1.0195 |     0.9372 |  -0.118279\n  350 |   0.018044 |     1.0241 |     0.9690 |  -0.105598\n  400 |   0.008188 |     1.0250 |     0.9893 |  -0.097477\n  450 |   0.004138 |     1.0246 |     1.0022 |  -0.092026\n  500 |   0.002439 |     1.0239 |     1.0104 |  -0.088173\n  550 |   0.001710 |     1.0230 |     1.0156 |  -0.085299\n  600 |   0.001384 |     1.0223 |     1.0188 |  -0.083036\n  650 |   0.001227 |     1.0217 |     1.0207 |  -0.081161\n  700 |   0.001142 |     1.0212 |     1.0218 |  -0.079538\n  750 |   0.001088 |     1.0207 |     1.0224 |  -0.078080\n  800 |   0.001046 |     1.0203 |     1.0227 |  -0.076735\n  850 |   0.001011 |     1.0199 |     1.0227 |  -0.075468\n  900 |   0.000980 |     1.0196 |     1.0226 |  -0.074258\n  950 |   0.000949 |     1.0192 |     1.0225 |  -0.073089\n 1000 |   0.000921 |     1.0189 |     1.0222 |  -0.071954\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Hypothesis without b"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n# Add bias(b) to the matrix in the previous code\n\nx_data = [\n    [1., 1., 1., 1., 1.], # bias(b)\n    [1., 0., 3., 0., 5.], \n    [0., 2., 0., 4., 0.]\n]\ny_data  = [1, 2, 3, 4, 5]\n\nW = tf.Variable(tf.random.uniform((1, 3), -1.0, 1.0)) # change to [1, 3] and delete b\n\nlearning_rate = 0.001\noptimizer = tf.keras.optimizers.SGD(learning_rate)\n\nfor i in range(1000+1):\n    with tf.GradientTape() as tape:\n        hypothesis = tf.matmul(W, x_data) # don't have b\n        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n\n    grads = tape.gradient(cost, [W])\n    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))\n    if i % 50 == 0:\n        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\".format(\n            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))","execution_count":5,"outputs":[{"output_type":"stream","text":"    0 |  16.019751 |    -0.1985 |     0.3424 |    -0.6835\n   50 |   5.635924 |     0.0582 |     0.6809 |    -0.1215\n  100 |   2.141112 |     0.1997 |     0.8238 |     0.2356\n  150 |   0.862825 |     0.2786 |     0.8808 |     0.4641\n  200 |   0.367091 |     0.3227 |     0.9015 |     0.6112\n  250 |   0.167513 |     0.3468 |     0.9074 |     0.7064\n  300 |   0.085210 |     0.3593 |     0.9082 |     0.7684\n  350 |   0.050615 |     0.3649 |     0.9074 |     0.8090\n  400 |   0.035731 |     0.3663 |     0.9067 |     0.8359\n  450 |   0.029064 |     0.3651 |     0.9063 |     0.8539\n  500 |   0.025846 |     0.3624 |     0.9064 |     0.8661\n  550 |   0.024085 |     0.3587 |     0.9069 |     0.8746\n  600 |   0.022948 |     0.3544 |     0.9076 |     0.8807\n  650 |   0.022085 |     0.3497 |     0.9086 |     0.8852\n  700 |   0.021348 |     0.3449 |     0.9097 |     0.8887\n  750 |   0.020676 |     0.3400 |     0.9109 |     0.8916\n  800 |   0.020042 |     0.3350 |     0.9121 |     0.8940\n  850 |   0.019434 |     0.3301 |     0.9133 |     0.8960\n  900 |   0.018848 |     0.3252 |     0.9146 |     0.8979\n  950 |   0.018280 |     0.3203 |     0.9158 |     0.8997\n 1000 |   0.017730 |     0.3155 |     0.9171 |     0.9013\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Custom Gradient"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multi-variable linear regression (1)\n\nX = tf.constant([[1., 2.], \n                 [3., 4.]])\ny = tf.constant([[1.5], [3.5]])\n\nW = tf.Variable(tf.random.normal((2, 1)))\nb = tf.Variable(tf.random.normal((1,)))\n\n# Create an optimizer\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n\nn_epoch = 1000+1\nprint(\"epoch | cost\")\nfor i in range(n_epoch):\n    # Use tf.GradientTape() to record the gradient of the cost function\n    with tf.GradientTape() as tape:\n        y_pred = tf.matmul(X, W) + b\n        cost = tf.reduce_mean(tf.square(y_pred - y))\n\n    # calculates the gradients of the loss\n    grads = tape.gradient(cost, [W, b])\n    \n    # updates parameters (W and b)\n    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n    if i % 50 == 0:\n        print(\"{:5} | {:10.6f}\".format(i, cost.numpy()))","execution_count":6,"outputs":[{"output_type":"stream","text":"epoch | cost\n    0 |  21.726822\n   50 |   0.258689\n  100 |   0.176868\n  150 |   0.120926\n  200 |   0.082678\n  250 |   0.056528\n  300 |   0.038649\n  350 |   0.026424\n  400 |   0.018067\n  450 |   0.012352\n  500 |   0.008445\n  550 |   0.005774\n  600 |   0.003948\n  650 |   0.002699\n  700 |   0.001845\n  750 |   0.001262\n  800 |   0.000863\n  850 |   0.000590\n  900 |   0.000403\n  950 |   0.000276\n 1000 |   0.000189\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Predicting Exam Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(0)  # for reproducibility","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data and label\nx1 = [ 73.,  93.,  89.,  96.,  73.]\nx2 = [ 80.,  88.,  91.,  98.,  66.]\nx3 = [ 75.,  93.,  90., 100.,  70.]\nY  = [152., 185., 180., 196., 142.]\n\n# weights\nw1 = tf.Variable(10.)\nw2 = tf.Variable(10.)\nw3 = tf.Variable(10.)\nb  = tf.Variable(10.)\n\nlearning_rate = 0.000001\n\nfor i in range(1000+1):\n    # tf.GradientTape() to record the gradient of the cost function\n    with tf.GradientTape() as tape:\n        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n    # calculates the gradients of the cost\n    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n    \n    # update w1,w2,w3 and b\n    w1.assign_sub(learning_rate * w1_grad)\n    w2.assign_sub(learning_rate * w2_grad)\n    w3.assign_sub(learning_rate * w3_grad)\n    b.assign_sub(learning_rate * b_grad)\n\n    if i % 50 == 0:\n        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))","execution_count":8,"outputs":[{"output_type":"stream","text":"    0 | 5793889.5000\n   50 |   64291.1562\n  100 |     715.2903\n  150 |       9.8461\n  200 |       2.0152\n  250 |       1.9252\n  300 |       1.9210\n  350 |       1.9177\n  400 |       1.9145\n  450 |       1.9114\n  500 |       1.9081\n  550 |       1.9050\n  600 |       1.9018\n  650 |       1.8986\n  700 |       1.8955\n  750 |       1.8923\n  800 |       1.8892\n  850 |       1.8861\n  900 |       1.8829\n  950 |       1.8798\n 1000 |       1.8767\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### MLR - tf.random_normal()"},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = [ 73.,  93.,  89.,  96.,  73.]\nx2 = [ 80.,  88.,  91.,  98.,  66.]\nx3 = [ 75.,  93.,  90., 100.,  70.]\nY  = [152., 185., 180., 196., 142.]\n\n# random weights\nw1 = tf.Variable(tf.random.normal((1,)))\nw2 = tf.Variable(tf.random.normal((1,)))\nw3 = tf.Variable(tf.random.normal((1,)))\nb  = tf.Variable(tf.random.normal((1,)))\n\nlearning_rate = 0.000001\n\nfor i in range(1000+1):\n    # tf.GradientTape() to record the gradient of the cost function\n    with tf.GradientTape() as tape:\n        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n    # calculates the gradients of the cost\n    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n    \n    # update w1,w2,w3 and b\n    w1.assign_sub(learning_rate * w1_grad)\n    w2.assign_sub(learning_rate * w2_grad)\n    w3.assign_sub(learning_rate * w3_grad)\n    b.assign_sub(learning_rate * b_grad)\n\n    if i % 50 == 0:\n        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))","execution_count":9,"outputs":[{"output_type":"stream","text":"    0 |   11325.9121\n   50 |     135.3618\n  100 |      11.1817\n  150 |       9.7940\n  200 |       9.7687\n  250 |       9.7587\n  300 |       9.7489\n  350 |       9.7389\n  400 |       9.7292\n  450 |       9.7194\n  500 |       9.7096\n  550 |       9.6999\n  600 |       9.6903\n  650 |       9.6806\n  700 |       9.6709\n  750 |       9.6612\n  800 |       9.6517\n  850 |       9.6421\n  900 |       9.6325\n  950 |       9.6229\n 1000 |       9.6134\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### MLR - using matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.array([\n    # X1,   X2,    X3,   y\n    [ 73.,  80.,  75., 152. ],\n    [ 93.,  88.,  93., 185. ],\n    [ 89.,  91.,  90., 180. ],\n    [ 96.,  98., 100., 196. ],\n    [ 73.,  66.,  70., 142. ]\n], dtype=np.float32)\n\n# slice data\nX = data[:, :-1]\ny = data[:, [-1]]\n\nW = tf.Variable(tf.random.normal((3, 1)))\nb = tf.Variable(tf.random.normal((1,)))\n\nlearning_rate = 0.000001\n\n# hypothesis, prediction function\ndef predict(X):\n    return tf.matmul(X, W) + b\n\nprint(\"epoch | cost\")\n\nn_epochs = 2000\nfor i in range(n_epochs+1):\n    # tf.GradientTape() to record the gradient of the cost function\n    with tf.GradientTape() as tape:\n        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n\n    # calculates the gradients of the loss\n    W_grad, b_grad = tape.gradient(cost, [W, b])\n\n    # updates parameters (W and b)\n    W.assign_sub(learning_rate * W_grad)\n    b.assign_sub(learning_rate * b_grad)\n    \n    if i % 100 == 0:\n        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))","execution_count":11,"outputs":[{"output_type":"stream","text":"epoch | cost\n    0 |  5455.5889\n  100 |    31.7444\n  200 |    30.9327\n  300 |    30.7893\n  400 |    30.6467\n  500 |    30.5053\n  600 |    30.3644\n  700 |    30.2241\n  800 |    30.0849\n  900 |    29.9462\n 1000 |    29.8081\n 1100 |    29.6711\n 1200 |    29.5347\n 1300 |    29.3990\n 1400 |    29.2640\n 1500 |    29.1298\n 1600 |    28.9960\n 1700 |    28.8632\n 1800 |    28.7312\n 1900 |    28.5995\n 2000 |    28.4689\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"W.numpy()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"array([[ 1.3685762],\n       [ 2.104772 ],\n       [-1.4229949]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"b.numpy()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"array([-1.1783521], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.matmul(X, W) + b","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\narray([[160.38489],\n       [178.98067],\n       [184.08965],\n       [194.17314],\n       [138.03304]], dtype=float32)>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y #labels (actual values)","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"[152.0, 185.0, 180.0, 196.0, 142.0]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(X).numpy() #prediction (resuls)","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"array([[160.38489],\n       [178.98067],\n       [184.08965],\n       [194.17314],\n       [138.03304]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction on new data(matrix)\npredict([[ 89.,  95.,  92.],[ 84.,  92.,  85.]]).numpy()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"array([[189.66275],\n       [186.46652]], dtype=float32)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## MLR with tf.keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lab 4 Multi-variable linear regression\nimport tensorflow as tf\nimport numpy as np\n\nx_data = [[73., 80., 75.],\n          [93., 88., 93.],\n          [89., 91., 90.],\n          [96., 98., 100.],\n          [73., 66., 70.]]\ny_data = [[152.],\n          [185.],\n          [180.],\n          [196.],\n          [142.]]\n\ntf.model = tf.keras.Sequential()\n\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=3))  # input_dim=3 gives multi-variable regression\ntf.model.add(tf.keras.layers.Activation('linear'))  # this line can be omitted, as linear activation is default\n# advanced reading https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n\ntf.model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-5))\ntf.model.summary()\nhistory = tf.model.fit(x_data, y_data, epochs=100)\n\ny_predict = tf.model.predict(np.array([[72., 93., 90.]]))\nprint(y_predict)","execution_count":19,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 1)                 4         \n_________________________________________________________________\nactivation (Activation)      (None, 1)                 0         \n=================================================================\nTotal params: 4\nTrainable params: 4\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/100\n1/1 [==============================] - 0s 2ms/step - loss: 53213.4492\nEpoch 2/100\n1/1 [==============================] - 0s 1ms/step - loss: 16683.7227\nEpoch 3/100\n1/1 [==============================] - 0s 1ms/step - loss: 5233.5894\nEpoch 4/100\n1/1 [==============================] - 0s 1ms/step - loss: 1644.5817\nEpoch 5/100\n1/1 [==============================] - 0s 1ms/step - loss: 519.6177\nEpoch 6/100\n1/1 [==============================] - 0s 1ms/step - loss: 167.0003\nEpoch 7/100\n1/1 [==============================] - 0s 1ms/step - loss: 56.4735\nEpoch 8/100\n1/1 [==============================] - 0s 1ms/step - loss: 21.8288\nEpoch 9/100\n1/1 [==============================] - 0s 2ms/step - loss: 10.9691\nEpoch 10/100\n1/1 [==============================] - 0s 1ms/step - loss: 7.5648\nEpoch 11/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.4973\nEpoch 12/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.1623\nEpoch 13/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0568\nEpoch 14/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0234\nEpoch 15/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0125\nEpoch 16/100\n1/1 [==============================] - 0s 2ms/step - loss: 6.0087\nEpoch 17/100\n1/1 [==============================] - 0s 2ms/step - loss: 6.0071\nEpoch 18/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0062\nEpoch 19/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0055\nEpoch 20/100\n1/1 [==============================] - 0s 2ms/step - loss: 6.0048\nEpoch 21/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0043\nEpoch 22/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0037\nEpoch 23/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0030\nEpoch 24/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0025\nEpoch 25/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0019\nEpoch 26/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0013\nEpoch 27/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0007\nEpoch 28/100\n1/1 [==============================] - 0s 1ms/step - loss: 6.0001\nEpoch 29/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9995\nEpoch 30/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9989\nEpoch 31/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9984\nEpoch 32/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9978\nEpoch 33/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9972\nEpoch 34/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9966\nEpoch 35/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9960\nEpoch 36/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9954\nEpoch 37/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9948\nEpoch 38/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9942\nEpoch 39/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9936\nEpoch 40/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9931\nEpoch 41/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9925\nEpoch 42/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9919\nEpoch 43/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9913\nEpoch 44/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9907\nEpoch 45/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9901\nEpoch 46/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9895\nEpoch 47/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9889\nEpoch 48/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9883\nEpoch 49/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9878\nEpoch 50/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9872\nEpoch 51/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9866\nEpoch 52/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9860\nEpoch 53/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9854\nEpoch 54/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9848\nEpoch 55/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9842\nEpoch 56/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9836\nEpoch 57/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9831\nEpoch 58/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9825\nEpoch 59/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9819\nEpoch 60/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9813\nEpoch 61/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9807\nEpoch 62/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9801\nEpoch 63/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9796\nEpoch 64/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9790\nEpoch 65/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9784\nEpoch 66/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9778\nEpoch 67/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9772\nEpoch 68/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9767\nEpoch 69/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9761\nEpoch 70/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9755\nEpoch 71/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9749\nEpoch 72/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9743\nEpoch 73/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9737\nEpoch 74/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9732\nEpoch 75/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9726\nEpoch 76/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9720\nEpoch 77/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9714\nEpoch 78/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9708\nEpoch 79/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9702\nEpoch 80/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9697\nEpoch 81/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9691\nEpoch 82/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9685\nEpoch 83/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9679\nEpoch 84/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9674\nEpoch 85/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9668\nEpoch 86/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9662\nEpoch 87/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9656\nEpoch 88/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9650\nEpoch 89/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9645\nEpoch 90/100\n1/1 [==============================] - 0s 3ms/step - loss: 5.9639\nEpoch 91/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9633\nEpoch 92/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9627\nEpoch 93/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9621\nEpoch 94/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9616\nEpoch 95/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9610\nEpoch 96/100\n","name":"stdout"},{"output_type":"stream","text":"1/1 [==============================] - 0s 1ms/step - loss: 5.9604\nEpoch 97/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9598\nEpoch 98/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9593\nEpoch 99/100\n1/1 [==============================] - 0s 2ms/step - loss: 5.9587\nEpoch 100/100\n1/1 [==============================] - 0s 1ms/step - loss: 5.9581\n[[183.35454]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}