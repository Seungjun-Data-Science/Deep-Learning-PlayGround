{"cells":[{"metadata":{},"cell_type":"markdown","source":"### **Downloading the Dataset**"},{"metadata":{},"cell_type":"markdown","source":"#### ML-100K"},{"metadata":{"trusted":true},"cell_type":"code","source":"  !wget \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n  !unzip ml-100k.zip\n  !ls","execution_count":1,"outputs":[{"output_type":"stream","text":"--2021-01-27 03:57:53--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\nResolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\nConnecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4924029 (4.7M) [application/zip]\nSaving to: ‘ml-100k.zip’\n\nml-100k.zip         100%[===================>]   4.70M  3.56MB/s    in 1.3s    \n\n2021-01-27 03:57:55 (3.56 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n\nArchive:  ml-100k.zip\n   creating: ml-100k/\n  inflating: ml-100k/allbut.pl       \n  inflating: ml-100k/mku.sh          \n  inflating: ml-100k/README          \n  inflating: ml-100k/u.data          \n  inflating: ml-100k/u.genre         \n  inflating: ml-100k/u.info          \n  inflating: ml-100k/u.item          \n  inflating: ml-100k/u.occupation    \n  inflating: ml-100k/u.user          \n  inflating: ml-100k/u1.base         \n  inflating: ml-100k/u1.test         \n  inflating: ml-100k/u2.base         \n  inflating: ml-100k/u2.test         \n  inflating: ml-100k/u3.base         \n  inflating: ml-100k/u3.test         \n  inflating: ml-100k/u4.base         \n  inflating: ml-100k/u4.test         \n  inflating: ml-100k/u5.base         \n  inflating: ml-100k/u5.test         \n  inflating: ml-100k/ua.base         \n  inflating: ml-100k/ua.test         \n  inflating: ml-100k/ub.base         \n  inflating: ml-100k/ub.test         \n__notebook_source__.ipynb  ml-100k  ml-100k.zip\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### ML-1M"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n!unzip ml-1m.zip\n!ls","execution_count":2,"outputs":[{"output_type":"stream","text":"--2021-01-27 03:57:58--  http://files.grouplens.org/datasets/movielens/ml-1m.zip\nResolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\nConnecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5917549 (5.6M) [application/zip]\nSaving to: ‘ml-1m.zip’\n\nml-1m.zip           100%[===================>]   5.64M  4.26MB/s    in 1.3s    \n\n2021-01-27 03:57:59 (4.26 MB/s) - ‘ml-1m.zip’ saved [5917549/5917549]\n\nArchive:  ml-1m.zip\n   creating: ml-1m/\n  inflating: ml-1m/movies.dat        \n  inflating: ml-1m/ratings.dat       \n  inflating: ml-1m/README            \n  inflating: ml-1m/users.dat         \n__notebook_source__.ipynb  ml-100k  ml-100k.zip  ml-1m\tml-1m.zip\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### **Importing Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.autograd import Variable","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We won't be using this dataset.\nmovies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\nusers = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\nratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparing the training set and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this dataset folder, there are 5 different train-test set pairs for k-fold cv // xx.base is the training set and xx.test is the test set\n# But we are just using one train-test set pair\n\ntraining_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')\ntraining_set = np.array(training_set, dtype = 'int') #pytorch can also work with arrays so transfer data to numpy array\n\ntest_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')\ntest_set = np.array(test_set, dtype = 'int') #pytorch can also work with arrays so transfer data to numpy array","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Getting the number of users and movies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The maximum user_id or movie_id can be either in training_set or test_set so have to do max using both\n\nnb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0]))) # index0 col = 1st column = user_id column\nnb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1]))) # index1 col = 2nd column = movid_id column","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting the data into an array with \"users in lines\" and \"movies in columns\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# For recommendation system, we need to struture the data in a particular manner\n# each movie has to correspond to each column/variable and each observation/line corresponds to each user(1st line = user1, 2nd line = user 2 ....)\n\n# Creating list of lists = what torch expects\ndef convert(data):\n    new_data = []\n    for id_users in range(1, nb_users + 1):\n        id_movies = data[:, 1] [data[:, 0] == id_users] # all the movie IDs each user watched\n        id_ratings = data[:, 2] [data[:, 0] == id_users] # all the ratings associated with those movies in the previous line\n        ratings = np.zeros(nb_movies)\n        ratings[id_movies - 1] = id_ratings\n        new_data.append(list(ratings))\n    return new_data\n\ntraining_set = convert(training_set)\ntest_set = convert(test_set)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting the data into Torch Tensors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch = multi dimensional matrix which is way more efficient than numpy arrays for most deep learning operations\n\ntraining_set = torch.FloatTensor(training_set)\ntest_set = torch.FloatTensor(test_set)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting the ratings into binary ratings (1 liked) or 0 (not liked)"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set[training_set == 0] = -1\ntraining_set[training_set == 1] = 0\ntraining_set[training_set == 2] = 0\ntraining_set[training_set >= 3] = 1\n\ntest_set[test_set == 0] = -1\ntest_set[test_set == 1] = 0\ntest_set[test_set == 2] = 0\ntest_set[test_set >= 3] = 1","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Architecture of NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stacked AutoEncoder\n\nclass SAE(nn.Module):\n    def __init__(self, ):\n        super(SAE, self).__init__() #super allows inheritance\n        \n        self.fc1 = nn.Linear(nb_movies, 20) \n        #(number of features==> in this case # of movies, number of nodes/neurons in the first hidden layer==> can be experimented with various numbers)\n        \n        self.fc2 = nn.Linear(20, 10) # 1st hidden layer - 2nd hidden layer full connection: 20 nodes in first hidden layer, so first number in Linear() is 20\n        # 2nd hidden layer number goes into the 2nd parameter of Linear()==> here choose 10 (can be tinkered/experimented)\n        \n        self.fc3 = nn.Linear(10, 20) # 2nd hidden layer - 3rd hidden layer full connection: 10 nodes in second hidden layer, so first number in Linear() is 10\n        # 3rd hidden layer number goes into the 2nd parameter of Linear()==> here choose 20 (can be tinkered/experimented)\n        \n        self.fc4 = nn.Linear(20, nb_movies)\n        \n        self.activation = nn.Sigmoid()\n    \n    def forward(self, x): #encoding --- and --- decoding\n        x = self.activation(self.fc1(x)) \n        x = self.activation(self.fc2(x)) \n        x = self.activation(self.fc3(x)) \n        x = self.fc4(x)\n        return x\n    \nsae = SAE()\ncriterion = nn.MSELoss() #Mean Squared Error\noptimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5) #adam? #RMSprop? ==> experimentation/tinkering possible","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training SAE"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"nb_epoch = 200\n\nfor epoch in range(1, nb_epoch + 1): #loop over epochs\n    train_loss = 0\n    s = 0. #keeping track of number of users who rated at least one movie\n    \n    for id_user in range(nb_users): #within that epoch loop, we loop over all users\n        \n        input = Variable(training_set[id_user]).unsqueeze(0) \n        #don't accept vector of single dimension.... so need to add some batch for new dimension\n        # 0 is the index for which new dimension will be created\n        \n        target = input.clone() #copy our inputs\n        \n        if torch.sum(target.data > 0) > 0: #target.data ==> all the ratings #checking if a user has at least one movie he/she rated\n            output = sae(input) #the vector of predicted ratings\n            target.require_grad = False #make sure we don't calculate the gradient with respect to the target (calculate gradient only for input)\n            output[target == 0] = 0 #exclude movies which don't have rating info(rating==0) for optimization\n            loss = criterion(output, target) #predicted ratings, real/true ratings\n            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) #1e-10: making sure denominator is not 0\n            loss.backward() #in which direction do we need to adjust our weights? (increase or decrease weights?)\n            \n            train_loss += np.sqrt(loss.data * mean_corrector) #loss data = mean error #multiply mean_corrector for adjustment\n            s += 1. #number of users who rated at least one movie\n            optimizer.step() #intensity/amount of change in weights\n    print('epoch: '+str(epoch)+'loss: '+ str(train_loss/s))","execution_count":12,"outputs":[{"output_type":"stream","text":"epoch: 1loss: tensor(2.2468)\nepoch: 2loss: tensor(2.1009)\nepoch: 3loss: tensor(2.0749)\nepoch: 4loss: tensor(2.0524)\nepoch: 5loss: tensor(2.0395)\nepoch: 6loss: tensor(2.0342)\nepoch: 7loss: tensor(2.0293)\nepoch: 8loss: tensor(2.0254)\nepoch: 9loss: tensor(2.0200)\nepoch: 10loss: tensor(2.0154)\nepoch: 11loss: tensor(2.0087)\nepoch: 12loss: tensor(2.0030)\nepoch: 13loss: tensor(1.9946)\nepoch: 14loss: tensor(1.9867)\nepoch: 15loss: tensor(1.9565)\nepoch: 16loss: tensor(1.9228)\nepoch: 17loss: tensor(1.9077)\nepoch: 18loss: tensor(1.8923)\nepoch: 19loss: tensor(1.8912)\nepoch: 20loss: tensor(1.8853)\nepoch: 21loss: tensor(1.8781)\nepoch: 22loss: tensor(1.8781)\nepoch: 23loss: tensor(1.8764)\nepoch: 24loss: tensor(1.8730)\nepoch: 25loss: tensor(1.8712)\nepoch: 26loss: tensor(1.8695)\nepoch: 27loss: tensor(1.8679)\nepoch: 28loss: tensor(1.8664)\nepoch: 29loss: tensor(1.8661)\nepoch: 30loss: tensor(1.8635)\nepoch: 31loss: tensor(1.8622)\nepoch: 32loss: tensor(1.8627)\nepoch: 33loss: tensor(1.8602)\nepoch: 34loss: tensor(1.8609)\nepoch: 35loss: tensor(1.8580)\nepoch: 36loss: tensor(1.8578)\nepoch: 37loss: tensor(1.8571)\nepoch: 38loss: tensor(1.8566)\nepoch: 39loss: tensor(1.8550)\nepoch: 40loss: tensor(1.8546)\nepoch: 41loss: tensor(1.8536)\nepoch: 42loss: tensor(1.8532)\nepoch: 43loss: tensor(1.8535)\nepoch: 44loss: tensor(1.8519)\nepoch: 45loss: tensor(1.8519)\nepoch: 46loss: tensor(1.8505)\nepoch: 47loss: tensor(1.8499)\nepoch: 48loss: tensor(1.8497)\nepoch: 49loss: tensor(1.8491)\nepoch: 50loss: tensor(1.8473)\nepoch: 51loss: tensor(1.8470)\nepoch: 52loss: tensor(1.8464)\nepoch: 53loss: tensor(1.8457)\nepoch: 54loss: tensor(1.8448)\nepoch: 55loss: tensor(1.8452)\nepoch: 56loss: tensor(1.8455)\nepoch: 57loss: tensor(1.8454)\nepoch: 58loss: tensor(1.8448)\nepoch: 59loss: tensor(1.8453)\nepoch: 60loss: tensor(1.8446)\nepoch: 61loss: tensor(1.8434)\nepoch: 62loss: tensor(1.8437)\nepoch: 63loss: tensor(1.8427)\nepoch: 64loss: tensor(1.8424)\nepoch: 65loss: tensor(1.8414)\nepoch: 66loss: tensor(1.8413)\nepoch: 67loss: tensor(1.8412)\nepoch: 68loss: tensor(1.8402)\nepoch: 69loss: tensor(1.8397)\nepoch: 70loss: tensor(1.8401)\nepoch: 71loss: tensor(1.8399)\nepoch: 72loss: tensor(1.8391)\nepoch: 73loss: tensor(1.8393)\nepoch: 74loss: tensor(1.8387)\nepoch: 75loss: tensor(1.8385)\nepoch: 76loss: tensor(1.8377)\nepoch: 77loss: tensor(1.8373)\nepoch: 78loss: tensor(1.8371)\nepoch: 79loss: tensor(1.8370)\nepoch: 80loss: tensor(1.8370)\nepoch: 81loss: tensor(1.8367)\nepoch: 82loss: tensor(1.8367)\nepoch: 83loss: tensor(1.8362)\nepoch: 84loss: tensor(1.8365)\nepoch: 85loss: tensor(1.8355)\nepoch: 86loss: tensor(1.8353)\nepoch: 87loss: tensor(1.8353)\nepoch: 88loss: tensor(1.8351)\nepoch: 89loss: tensor(1.8351)\nepoch: 90loss: tensor(1.8357)\nepoch: 91loss: tensor(1.8347)\nepoch: 92loss: tensor(1.8345)\nepoch: 93loss: tensor(1.8340)\nepoch: 94loss: tensor(1.8344)\nepoch: 95loss: tensor(1.8339)\nepoch: 96loss: tensor(1.8345)\nepoch: 97loss: tensor(1.8333)\nepoch: 98loss: tensor(1.8340)\nepoch: 99loss: tensor(1.8333)\nepoch: 100loss: tensor(1.8340)\nepoch: 101loss: tensor(1.8341)\nepoch: 102loss: tensor(1.8340)\nepoch: 103loss: tensor(1.8328)\nepoch: 104loss: tensor(1.8327)\nepoch: 105loss: tensor(1.8320)\nepoch: 106loss: tensor(1.8337)\nepoch: 107loss: tensor(1.8316)\nepoch: 108loss: tensor(1.8328)\nepoch: 109loss: tensor(1.8316)\nepoch: 110loss: tensor(1.8316)\nepoch: 111loss: tensor(1.8316)\nepoch: 112loss: tensor(1.8325)\nepoch: 113loss: tensor(1.8312)\nepoch: 114loss: tensor(1.8317)\nepoch: 115loss: tensor(1.8310)\nepoch: 116loss: tensor(1.8319)\nepoch: 117loss: tensor(1.8307)\nepoch: 118loss: tensor(1.8316)\nepoch: 119loss: tensor(1.8308)\nepoch: 120loss: tensor(1.8313)\nepoch: 121loss: tensor(1.8304)\nepoch: 122loss: tensor(1.8314)\nepoch: 123loss: tensor(1.8304)\nepoch: 124loss: tensor(1.8308)\nepoch: 125loss: tensor(1.8295)\nepoch: 126loss: tensor(1.8310)\nepoch: 127loss: tensor(1.8299)\nepoch: 128loss: tensor(1.8306)\nepoch: 129loss: tensor(1.8292)\nepoch: 130loss: tensor(1.8310)\nepoch: 131loss: tensor(1.8295)\nepoch: 132loss: tensor(1.8308)\nepoch: 133loss: tensor(1.8292)\nepoch: 134loss: tensor(1.8303)\nepoch: 135loss: tensor(1.8289)\nepoch: 136loss: tensor(1.8304)\nepoch: 137loss: tensor(1.8287)\nepoch: 138loss: tensor(1.8304)\nepoch: 139loss: tensor(1.8287)\nepoch: 140loss: tensor(1.8294)\nepoch: 141loss: tensor(1.8280)\nepoch: 142loss: tensor(1.8293)\nepoch: 143loss: tensor(1.8284)\nepoch: 144loss: tensor(1.8291)\nepoch: 145loss: tensor(1.8274)\nepoch: 146loss: tensor(1.8287)\nepoch: 147loss: tensor(1.8273)\nepoch: 148loss: tensor(1.8282)\nepoch: 149loss: tensor(1.8268)\nepoch: 150loss: tensor(1.8275)\nepoch: 151loss: tensor(1.8286)\nepoch: 152loss: tensor(1.8245)\nepoch: 153loss: tensor(1.8242)\nepoch: 154loss: tensor(1.8232)\nepoch: 155loss: tensor(1.8253)\nepoch: 156loss: tensor(1.8237)\nepoch: 157loss: tensor(1.8265)\nepoch: 158loss: tensor(1.8255)\nepoch: 159loss: tensor(1.8248)\nepoch: 160loss: tensor(1.8255)\nepoch: 161loss: tensor(1.8251)\nepoch: 162loss: tensor(1.8256)\nepoch: 163loss: tensor(1.8245)\nepoch: 164loss: tensor(1.8252)\nepoch: 165loss: tensor(1.8241)\nepoch: 166loss: tensor(1.8246)\nepoch: 167loss: tensor(1.8237)\nepoch: 168loss: tensor(1.8235)\nepoch: 169loss: tensor(1.8252)\nepoch: 170loss: tensor(1.8236)\nepoch: 171loss: tensor(1.8232)\nepoch: 172loss: tensor(1.8230)\nepoch: 173loss: tensor(1.8230)\nepoch: 174loss: tensor(1.8228)\nepoch: 175loss: tensor(1.8229)\nepoch: 176loss: tensor(1.8215)\nepoch: 177loss: tensor(1.8209)\nepoch: 178loss: tensor(1.8208)\nepoch: 179loss: tensor(1.8214)\nepoch: 180loss: tensor(1.8219)\nepoch: 181loss: tensor(1.8206)\nepoch: 182loss: tensor(1.8212)\nepoch: 183loss: tensor(1.8208)\nepoch: 184loss: tensor(1.8209)\nepoch: 185loss: tensor(1.8191)\nepoch: 186loss: tensor(1.8197)\nepoch: 187loss: tensor(1.8192)\nepoch: 188loss: tensor(1.8200)\nepoch: 189loss: tensor(1.8205)\nepoch: 190loss: tensor(1.8194)\nepoch: 191loss: tensor(1.8195)\nepoch: 192loss: tensor(1.8197)\nepoch: 193loss: tensor(1.8187)\nepoch: 194loss: tensor(1.8191)\nepoch: 195loss: tensor(1.8181)\nepoch: 196loss: tensor(1.8183)\nepoch: 197loss: tensor(1.8181)\nepoch: 198loss: tensor(1.8186)\nepoch: 199loss: tensor(1.8178)\nepoch: 200loss: tensor(1.8154)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Testing RBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss = 0\ns = 0.\nfor id_user in range(nb_users):\n    input = Variable(training_set[id_user]).unsqueeze(0)\n    target = Variable(test_set[id_user]).unsqueeze(0)\n    if torch.sum(target.data > 0) > 0:\n        output = sae(input)\n        target.require_grad = False\n        output[target == 0] = 0\n        loss = criterion(output, target)\n        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n        test_loss += np.sqrt(loss.data*mean_corrector)\n        s += 1.\nprint('test loss: '+str(test_loss/s))","execution_count":13,"outputs":[{"output_type":"stream","text":"test loss: tensor(2.2516)\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}